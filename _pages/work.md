---
layout: archive
title: "Work"
permalink: /work/
author_profile: true
---

## AI and Data Science Summer Analyst at JPMorgan Chase
*Details forthcoming*

## Data Science Intern at Plastic Omnium
Throughout the Summer of 2021, I worked as a data scientist at [Plastic Omnium](https://www.plasticomnium.com/en/), the world's leading fuel tank manufacturer. I worked within the engineering and digital manufacturing department under Derek Naas's supervision. I was tasked with two main projects: 1) developing an algorithm that predicts the thickness of a given fuel tank and 2) assist in improving and validating an existing algorithm which predicts the weld quality of various components inside of a fuel tank. The motivation for both algorithms comes from a goal of reducing manual labor and destructive testing, which in turn save time and money. 

The first algorithm was really my own project - I was the sole perosn working on this one. As usual with any data science project, I first had to collect data. I used the company's software to pull data holding information for 100,000 fuel tanks. The data consisted of each tank's thickness at different areas and various features inlcuding machine parameters (e.g. cycle time and temperature), machine throughput, and fuel tank weight. Of course, with all this data, I had to do some serious data cleaning; this called for changing data formats, converting values, the merging of various datasets, dealing with missing data, and normalization. The next steps I took were making some preliminary plots and statistical calculations to see if there are any initial patterns I can spot before building a model. Then I decided to write this thickness prediction algorithm in Julia. Following, I tested three different models: a neural network, a random forest regression, and a decision tree. Of course, each model calls for its own variations within itself as adjusting hyperparamters may give rise to different results. After the trainings and validation testings of each model, we decided to move forward with a neural network as it performed the highest. With this, I was able to calculate feature importance to share with the engineering department which variables are most influential in predicting a fuel tank's thickness. Altogether, the implementation of this algorithm had projected savings of a few million dollars per year.

The other project I particpated in was the weld quality algorithm, a binary classification model that outputs "true" if a welding compnent is good quality and "false" otherwise. When I was brought in, an algorithm had already been written, though my boss wanted me to validate it. In other words, the algorithm had been trained and performed well on data it had already seen, but the company had yet to test it on fresh data coming from the manufacturing plant. So I went into the plant and collected welding data on tanks that had just been created to test whether or not the algorithm can predict if these tanks have proper welds. After runing the algorithm with this data as input, I found that the predictions were essentially arbitary, meaning that the algorithm could not accurately predict the weld quality; tanks with good welds were labelled as bad and vice versa. Consequently, this hinted that the model hadn't been properly trained on a variety of data and, unfortunaltely, caused a biased algorithm. Though, this wasn't really the previous data scientist's fault, but rather this happened because the company created very few bad tanks each year, giving rise to an oversampled dataset consisting of primarily good weld tanks. Thus, I suggested to my boss, the plant director, and the quality manager that, occasionally, they manually create a fault within a batch of tanks, allowing data scientists to train their models with data containing, roughly, an equal number of successes and failures resulting from different cases.
